{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Signal Generator - Training Pipeline\n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Downloading market data\n",
    "2. Engineering features\n",
    "3. Training ML models\n",
    "4. Generating trading signals\n",
    "5. Backtesting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from features import download_data, engineer_features, prepare_features_for_training\n",
    "from model import time_series_split, train_random_forest, train_xgboost, get_feature_importance, evaluate_model\n",
    "from backtest import generate_signals, backtest_strategy, plot_equity_curve, plot_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for SPY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaspa\\Documents\\ml-signal-generator\\src\\features.py:27: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
      "\n",
      "1 Failed download:\n",
      "['SPY']: DNSError('Failed to perform, curl: (6) Could not resolve host: query1.finance.yahoo.com. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 0 days of data\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m data = download_data(TICKER, START_DATE, END_DATE)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m days of data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDate range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.index[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m data.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gaspa\\Documents\\ml-signal-generator\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5401\u001b[39m, in \u001b[36mIndex.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   5398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[32m   5399\u001b[39m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[32m   5400\u001b[39m     key = com.cast_scalar_indexer(key)\n\u001b[32m-> \u001b[39m\u001b[32m5401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   5404\u001b[39m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[32m   5405\u001b[39m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[32m   5406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_slice(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gaspa\\Documents\\ml-signal-generator\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:398\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[33;03mThis getitem defers to the underlying array, which by-definition can\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[33;03monly handle list-likes, slices, and integer scalars\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    395\u001b[39m \u001b[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m result = cast(\u001b[33m\"\u001b[39m\u001b[33mUnion[Self, DTScalarOrNaT]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lib.is_scalar(result):\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gaspa\\Documents\\ml-signal-generator\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:284\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\n\u001b[32m    279\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    280\u001b[39m     key: PositionalIndexer2D,\n\u001b[32m    281\u001b[39m ) -> Self | Any:\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_integer(key):\n\u001b[32m    283\u001b[39m         \u001b[38;5;66;03m# fast-path\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ndarray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    286\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._box_func(result)\n",
      "\u001b[31mIndexError\u001b[39m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TICKER = 'SPY'  # S&P 500 ETF\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2024-01-01'\n",
    "\n",
    "# Download OHLC data\n",
    "print(f\"Downloading data for {TICKER}...\")\n",
    "data = download_data(TICKER, START_DATE, END_DATE)\n",
    "print(f\"Downloaded {len(data)} days of data\")\n",
    "print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features...\n",
      "\n",
      "Features shape: (986, 6)\n",
      "Target distribution:\n",
      "target\n",
      "1    529\n",
      "0    457\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature columns: ['return_1d', 'return_5d', 'volatility_20d', 'ma_5d', 'ma_20d', 'ma_gap']\n"
     ]
    }
   ],
   "source": [
    "# Engineer features\n",
    "print(\"Engineering features...\")\n",
    "data_features = engineer_features(\n",
    "    data,\n",
    "    return_periods=[1, 5],\n",
    "    volatility_window=20,\n",
    "    ma_windows=[5, 20]\n",
    ")\n",
    "\n",
    "# Prepare features for training\n",
    "X, y = prepare_features_for_training(data_features)\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split (Time Series Aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 690 samples\n",
      "Validation set: 148 samples\n",
      "Test set: 148 samples\n"
     ]
    }
   ],
   "source": [
    "# Combine X and y for time series split\n",
    "data_combined = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Split data\n",
    "train, val, test = time_series_split(data_combined, train_size=0.7, val_size=0.15)\n",
    "\n",
    "X_train = train[X.columns]\n",
    "y_train = train['target']\n",
    "X_val = val[X.columns]\n",
    "y_val = val['target']\n",
    "X_test = test[X.columns]\n",
    "y_test = test['target']\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "\n",
      "Validation Metrics:\n",
      "  AUC: 0.5416\n",
      "  Train Accuracy: 0.9725\n",
      "  Validation Accuracy: 0.5473\n"
     ]
    }
   ],
   "source": [
    "# Choose model: 'random_forest' or 'xgboost'\n",
    "MODEL_TYPE = 'random_forest'  # Change to 'xgboost' to use XGBoost\n",
    "\n",
    "if MODEL_TYPE == 'random_forest':\n",
    "    print(\"Training Random Forest...\")\n",
    "    model, metrics = train_random_forest(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "elif MODEL_TYPE == 'xgboost':\n",
    "    print(\"Training XGBoost...\")\n",
    "    model, metrics = train_xgboost(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  AUC: {metrics['auc']:.4f}\")\n",
    "print(f\"  Train Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "print(f\"  Validation Accuracy: {metrics['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "          feature  importance\n",
      "0       return_1d    0.184743\n",
      "2  volatility_20d    0.171167\n",
      "1       return_5d    0.168430\n",
      "4          ma_20d    0.159738\n",
      "5          ma_gap    0.158479\n",
      "3           ma_5d    0.157443\n",
      "Feature importance plot saved to outputs/feature_importance.png\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance\n",
    "importance_df = get_feature_importance(model, list(X.columns))\n",
    "print(\"Feature Importance:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(importance_df, '../outputs/feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "  AUC: 0.4685\n",
      "  Accuracy: 0.5135\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.14      0.20        63\n",
      "           1       0.55      0.79      0.65        85\n",
      "\n",
      "    accuracy                           0.51       148\n",
      "   macro avg       0.44      0.47      0.43       148\n",
      "weighted avg       0.46      0.51      0.46       148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_results = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  AUC: {test_results['auc']:.4f}\")\n",
    "print(f\"  Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_results['y_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Trading Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal Statistics:\n",
      "  Total signals: 98 out of 147 days\n",
      "  Signal rate: 66.67%\n",
      "\n",
      "Test returns range: 2023-05-31 00:00:00 to 2023-12-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Generate signals on test set\n",
    "SIGNAL_THRESHOLD = 0.55  # Probability threshold for signal generation\n",
    "\n",
    "# Create signals as Series with X_test index for proper alignment\n",
    "signals = pd.Series(\n",
    "    generate_signals(test_results['y_pred_proba'], threshold=SIGNAL_THRESHOLD),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Get actual returns for backtesting\n",
    "test_returns = data_features.loc[X_test.index, 'next_return'].dropna()\n",
    "\n",
    "# Align signals with test_returns (remove rows where returns are NaN)\n",
    "signals = signals.loc[test_returns.index]\n",
    "\n",
    "print(f\"Signal Statistics:\")\n",
    "print(f\"  Total signals: {signals.sum()} out of {len(signals)} days\")\n",
    "print(f\"  Signal rate: {signals.mean()*100:.2f}%\")\n",
    "print(f\"\\nTest returns range: {test_returns.index[0]} to {test_returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Backtest Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest Performance Metrics:\n",
      "  Total Return: 2.89%\n",
      "  Annualized Return: 5.00%\n",
      "  Volatility: 9.67%\n",
      "  Sharpe Ratio: 0.52\n",
      "  Max Drawdown: -11.16%\n",
      "  Win Rate: 55.67%\n",
      "  Total Trades: 98\n"
     ]
    }
   ],
   "source": [
    "# Align signals with returns (already aligned in previous cell)\n",
    "aligned_data = pd.DataFrame({\n",
    "    'signals': signals.values,\n",
    "    'returns': test_returns.values\n",
    "}, index=signals.index)\n",
    "\n",
    "# Backtest\n",
    "equity, metrics = backtest_strategy(\n",
    "    aligned_data['signals'].values,\n",
    "    aligned_data['returns'].values,\n",
    "    initial_capital=10000.0\n",
    ")\n",
    "\n",
    "# Create equity series with dates\n",
    "# Handle both Series and array returns from backtest_strategy\n",
    "if isinstance(equity, pd.Series):\n",
    "    equity_series = equity.reindex(aligned_data.index)\n",
    "else:\n",
    "    equity_series = pd.Series(equity, index=aligned_data.index)\n",
    "\n",
    "print(\"\\nBacktest Performance Metrics:\")\n",
    "print(f\"  Total Return: {metrics['total_return_pct']:.2f}%\")\n",
    "print(f\"  Annualized Return: {metrics['annualized_return_pct']:.2f}%\")\n",
    "print(f\"  Volatility: {metrics['volatility_pct']:.2f}%\")\n",
    "print(f\"  Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {metrics['max_drawdown_pct']:.2f}%\")\n",
    "print(f\"  Win Rate: {metrics['win_rate_pct']:.2f}%\")\n",
    "print(f\"  Total Trades: {metrics['total_trades']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equity curve saved to outputs/equity_curve.png\n"
     ]
    }
   ],
   "source": [
    "# Plot equity curve\n",
    "plot_equity_curve(equity_series, '../outputs/equity_curve.png', 'Strategy Equity Curve')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
