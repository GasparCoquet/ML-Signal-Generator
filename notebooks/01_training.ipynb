{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Signal Generator - Training Pipeline\n",
    "\n",
    "This notebook demonstrates the complete pipeline for:\n",
    "1. Downloading market data\n",
    "2. Engineering features\n",
    "3. Training ML models\n",
    "4. Generating trading signals\n",
    "5. Backtesting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the project root directory (parent of notebooks directory)\n",
    "current_dir = Path(os.getcwd())\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    # If running from project root, notebooks should be in notebooks/ subdirectory\n",
    "    project_root = current_dir\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env_path = project_root / '.env'\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"✓ Loaded environment variables from .env\")\n",
    "else:\n",
    "    print(f\"⚠️  No .env file found. Using defaults. Create .env from .env.example if needed.\")\n",
    "\n",
    "# Add src to path\n",
    "src_path = project_root / 'src'\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Set output directory\n",
    "output_dir = project_root / 'outputs'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display  # For better DataFrame display in Jupyter\n",
    "\n",
    "from src.features import download_data, engineer_features, prepare_features_for_training\n",
    "from src.model import time_series_split, train_random_forest, train_xgboost, get_feature_importance, evaluate_model\n",
    "from src.backtest import generate_signals, backtest_strategy, plot_equity_curve, plot_feature_importance\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TICKER = 'SPY'  # S&P 500 ETF\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2024-01-01'\n",
    "\n",
    "# API Configuration (loads from .env file, with fallback to defaults)\n",
    "# Options: 'yfinance' (default, free but rate limited), 'alpha_vantage' (free with API key)\n",
    "# NOTE: If Alpha Vantage gives errors, you can override here: API_SOURCE = 'yfinance'\n",
    "API_SOURCE = os.getenv('API_SOURCE', 'yfinance')  # Load from .env or use default\n",
    "\n",
    "# Get API key from .env based on source\n",
    "if API_SOURCE == 'alpha_vantage':\n",
    "    API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')\n",
    "else:\n",
    "    API_KEY = None  # yfinance doesn't need an API key\n",
    "\n",
    "# Display API configuration\n",
    "print(f\"API Source: {API_SOURCE}\")\n",
    "if API_KEY:\n",
    "    print(f\"API Key: {'*' * (len(API_KEY) - 4) + API_KEY[-4:]}\")  # Show only last 4 chars\n",
    "else:\n",
    "    print(\"API Key: Not set (using yfinance or not required)\")\n",
    "    if API_SOURCE == 'alpha_vantage':\n",
    "        print(f\"⚠️  WARNING: {API_SOURCE} requires an API key but none was found!\")\n",
    "        print(\"   Either add API key to .env or set API_SOURCE='yfinance'\")\n",
    "\n",
    "# Quick fix: If Alpha Vantage fails, uncomment the line below to force yfinance:\n",
    "# API_SOURCE = 'yfinance'\n",
    "\n",
    "# Instructions for setting up .env:\n",
    "# 1. Copy .env.example to .env: cp .env.example .env\n",
    "# 2. Edit .env and add your API keys\n",
    "# 3. Get free API keys:\n",
    "#    - Alpha Vantage: https://www.alphavantage.co/support/#api-key\n",
    "\n",
    "# OPTION: Set to True to skip download and use sample data (for testing when rate limited)\n",
    "USE_SAMPLE_DATA = False  # Set to True if you're rate limited and want to continue\n",
    "\n",
    "# Try to load from saved file first (checks both CSV and Excel)\n",
    "data_file_csv = project_root / 'data' / f'{TICKER}_{START_DATE}_{END_DATE}.csv'\n",
    "data_file_xlsx = project_root / 'data' / f'{TICKER}_{START_DATE}_{END_DATE}.xlsx'\n",
    "data = None\n",
    "\n",
    "# Check for CSV first, then Excel\n",
    "if data_file_csv.exists():\n",
    "    print(f\"Loading saved data from {data_file_csv.name}...\")\n",
    "    try:\n",
    "        data = pd.read_csv(data_file_csv, index_col=0, parse_dates=True)\n",
    "        print(f\"✓ Loaded {len(data)} days of saved data from CSV\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading CSV: {e}\")\n",
    "        data = None\n",
    "elif data_file_xlsx.exists():\n",
    "    print(f\"Loading saved data from {data_file_xlsx.name}...\")\n",
    "    try:\n",
    "        data = pd.read_excel(data_file_xlsx, index_col=0, parse_dates=True)\n",
    "        print(f\"✓ Loaded {len(data)} days of saved data from Excel\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error loading Excel: {e}\")\n",
    "        data = None\n",
    "\n",
    "# If no saved data, try to download\n",
    "if data is None or data.empty:\n",
    "    print(f\"\\nDownloading data for {TICKER}...\")\n",
    "    import time\n",
    "    \n",
    "    max_retries = 3\n",
    "    retry_delay = 30  # seconds - increased to 30 seconds between retries\n",
    "    initial_wait = 60  # Wait 60 seconds before first attempt if recently rate limited\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        # Wait before first attempt if this is a retry (not the first attempt)\n",
    "        if attempt > 0:\n",
    "            wait_time = retry_delay * attempt\n",
    "            print(f\"Waiting {wait_time} seconds before retry attempt {attempt + 1}/{max_retries}...\")\n",
    "            time.sleep(wait_time)\n",
    "        \n",
    "        try:\n",
    "            data = download_data(TICKER, START_DATE, END_DATE, api_source=API_SOURCE, api_key=API_KEY)\n",
    "            \n",
    "            # Check if download was successful\n",
    "            if data.empty or len(data) == 0:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Will wait at start of next loop iteration\n",
    "                    print(f\"Download returned empty data. Will retry... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                    continue\n",
    "                else:\n",
    "                    raise ValueError(f\"Failed to download data for {TICKER} after {max_retries} attempts.\")\n",
    "            \n",
    "            # Save data for future use\n",
    "            data_dir = project_root / 'data'\n",
    "            data_dir.mkdir(exist_ok=True)\n",
    "            data.to_csv(data_file_csv)\n",
    "            print(f\"✓ Data saved to {data_file_csv.name} for future use\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            error_type = type(e).__name__\n",
    "            \n",
    "            # Check for rate limit errors (both in message and exception type)\n",
    "            is_rate_limit = (\n",
    "                'rate limit' in error_msg or \n",
    "                'too many requests' in error_msg or\n",
    "                'YFRateLimitError' in error_type or\n",
    "                'ratelimit' in error_msg\n",
    "            )\n",
    "            \n",
    "            if is_rate_limit:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Will wait at start of next loop iteration\n",
    "                    print(f\"Rate limit hit. Will retry with longer wait... (Attempt {attempt + 1}/{max_retries})\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"\\n❌ Rate limited after {max_retries} attempts.\")\n",
    "                    print(\"\\n\" + \"=\"*60)\n",
    "                    print(\"YAHOO FINANCE RATE LIMIT - SOLUTIONS:\")\n",
    "                    print(\"=\"*60)\n",
    "                    print(\"1. Wait 15-20 minutes, then run this cell again\")\n",
    "                    print(\"2. Try again later today (rate limits reset over time)\")\n",
    "                    print(\"3. Once data downloads successfully, it will be saved\")\n",
    "                    print(\"   and you won't need to download again\")\n",
    "                    print(\"=\"*60)\n",
    "                    raise ValueError(f\"Rate limited by Yahoo Finance. Please wait 15-20 minutes and try again.\")\n",
    "            else:\n",
    "                # For other errors, raise immediately\n",
    "                print(f\"Error: {e}\")\n",
    "                raise\n",
    "\n",
    "# If still no data and USE_SAMPLE_DATA is True, generate sample data\n",
    "if (data is None or data.empty) and USE_SAMPLE_DATA:\n",
    "    print(\"\\n⚠️  Using sample data (USE_SAMPLE_DATA=True)\")\n",
    "    print(\"This is synthetic data for testing purposes only!\")\n",
    "    \n",
    "    # Generate sample OHLC data\n",
    "    dates = pd.date_range(start=START_DATE, end=END_DATE, freq='D')\n",
    "    dates = dates[dates.weekday < 5]  # Only weekdays\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_days = len(dates)\n",
    "    base_price = 400.0\n",
    "    \n",
    "    # Generate realistic price movements\n",
    "    returns = np.random.normal(0.0005, 0.015, n_days)  # Daily returns\n",
    "    prices = base_price * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Generate OHLC from close prices\n",
    "    data = pd.DataFrame({\n",
    "        'Open': prices * (1 + np.random.normal(0, 0.002, n_days)),\n",
    "        'High': prices * (1 + np.abs(np.random.normal(0, 0.005, n_days))),\n",
    "        'Low': prices * (1 - np.abs(np.random.normal(0, 0.005, n_days))),\n",
    "        'Close': prices,\n",
    "        'Volume': np.random.randint(50000000, 200000000, n_days)\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Ensure High >= Close >= Low and High >= Open >= Low\n",
    "    data['High'] = data[['Open', 'High', 'Close']].max(axis=1) * 1.001\n",
    "    data['Low'] = data[['Open', 'Low', 'Close']].min(axis=1) * 0.999\n",
    "    \n",
    "    print(f\"✓ Generated {len(data)} days of sample data\")\n",
    "    print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "    print(f\"\\n⚠️  WARNING: This is synthetic data, not real market data!\")\n",
    "    print(f\"   Set USE_SAMPLE_DATA=False and wait 15-20 minutes to download real data.\")\n",
    "\n",
    "# Display data info\n",
    "if data is not None and not data.empty:\n",
    "    print(f\"\\n✓ Successfully loaded {len(data)} days of data\")\n",
    "    print(f\"Date range: {data.index[0]} to {data.index[-1]}\")\n",
    "    print(f\"\\nData preview:\")\n",
    "    display(data.head())  # Use display() to ensure it shows in Jupyter\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NO DATA AVAILABLE - OPTIONS:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"1. Wait 15-20 minutes, then run this cell again\")\n",
    "    print(\"2. Set USE_SAMPLE_DATA = True at the top of this cell to use sample data\")\n",
    "    print(\"   (for testing purposes only - not real market data)\")\n",
    "    print(\"3. Try again later today (rate limits reset over time)\")\n",
    "    print(\"=\"*70)\n",
    "    raise ValueError(\"No data available. Set USE_SAMPLE_DATA=True to continue with sample data, or wait and try downloading again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features\n",
    "print(\"Engineering features...\")\n",
    "data_features = engineer_features(\n",
    "    data,\n",
    "    return_periods=[1, 5],\n",
    "    volatility_window=20,\n",
    "    ma_windows=[5, 20]\n",
    ")\n",
    "\n",
    "# Prepare features for training\n",
    "X, y = prepare_features_for_training(data_features)\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split (Time Series Aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X and y for time series split\n",
    "data_combined = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Split data\n",
    "train, val, test = time_series_split(data_combined, train_size=0.7, val_size=0.15)\n",
    "\n",
    "X_train = train[X.columns]\n",
    "y_train = train['target']\n",
    "X_val = val[X.columns]\n",
    "y_val = val['target']\n",
    "X_test = test[X.columns]\n",
    "y_test = test['target']\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both models for comparison\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING BOTH MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\n1. Training Random Forest...\")\n",
    "model_rf, metrics_rf = train_random_forest(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nRandom Forest Validation Metrics:\")\n",
    "print(f\"  AUC: {metrics_rf['auc']:.4f}\")\n",
    "print(f\"  Train Accuracy: {metrics_rf['train_accuracy']:.4f}\")\n",
    "print(f\"  Validation Accuracy: {metrics_rf['val_accuracy']:.4f}\")\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"\\n2. Training XGBoost...\")\n",
    "model_xgb, metrics_xgb = train_xgboost(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nXGBoost Validation Metrics:\")\n",
    "print(f\"  AUC: {metrics_xgb['auc']:.4f}\")\n",
    "print(f\"  Train Accuracy: {metrics_xgb['train_accuracy']:.4f}\")\n",
    "print(f\"  Validation Accuracy: {metrics_xgb['val_accuracy']:.4f}\")\n",
    "\n",
    "# Model Comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON (Validation Set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} {'Random Forest':<20} {'XGBoost':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'AUC':<25} {metrics_rf['auc']:<20.4f} {metrics_xgb['auc']:<20.4f}\")\n",
    "print(f\"{'Train Accuracy':<25} {metrics_rf['train_accuracy']:<20.4f} {metrics_xgb['train_accuracy']:<20.4f}\")\n",
    "print(f\"{'Validation Accuracy':<25} {metrics_rf['val_accuracy']:<20.4f} {metrics_xgb['val_accuracy']:<20.4f}\")\n",
    "\n",
    "# Determine which model performs better on validation\n",
    "if metrics_rf['auc'] > metrics_xgb['auc']:\n",
    "    print(f\"\\n✓ Random Forest has better AUC on validation set\")\n",
    "    best_model_name = 'Random Forest'\n",
    "    best_model = model_rf\n",
    "    best_metrics = metrics_rf\n",
    "    print(f\"\\nUsing {best_model_name} for further analysis...\")\n",
    "elif metrics_xgb['auc'] > metrics_rf['auc']:\n",
    "    print(f\"\\n✓ XGBoost has better AUC on validation set\")\n",
    "    best_model_name = 'XGBoost'\n",
    "    best_model = model_xgb\n",
    "    best_metrics = metrics_xgb\n",
    "    print(f\"\\nUsing {best_model_name} for further analysis...\")\n",
    "else:\n",
    "    print(f\"\\n✓ Both models have similar AUC on validation set (no clear winner)\")\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for both models\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "importance_df_rf = get_feature_importance(model_rf, list(X.columns))\n",
    "importance_df_xgb = get_feature_importance(model_xgb, list(X.columns))\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(importance_df_rf)\n",
    "\n",
    "print(\"\\nXGBoost Feature Importance:\")\n",
    "print(importance_df_xgb)\n",
    "\n",
    "# Plot feature importance for both models\n",
    "# Ensure output_dir is defined (from Cell 1)\n",
    "if 'output_dir' not in globals():\n",
    "    from pathlib import Path\n",
    "    output_dir = Path('outputs')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "feature_importance_path_rf = str(output_dir / 'feature_importance_rf.png')\n",
    "feature_importance_path_xgb = str(output_dir / 'feature_importance_xgb.png')\n",
    "\n",
    "plot_feature_importance(importance_df_rf, feature_importance_path_rf)\n",
    "plot_feature_importance(importance_df_xgb, feature_importance_path_xgb)\n",
    "\n",
    "print(f\"\\n✓ Feature importance plots saved:\")\n",
    "print(f\"  - Random Forest: {feature_importance_path_rf}\")\n",
    "print(f\"  - XGBoost: {feature_importance_path_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST SET EVALUATION - BOTH MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "print(\"\\n1. Random Forest Test Set Metrics:\")\n",
    "test_results_rf = evaluate_model(model_rf, X_test, y_test)\n",
    "print(f\"  AUC: {test_results_rf['auc']:.4f}\")\n",
    "print(f\"  Accuracy: {test_results_rf['accuracy']:.4f}\")\n",
    "print(f\"\\n  Classification Report:\")\n",
    "print(classification_report(y_test, test_results_rf['y_pred']))\n",
    "\n",
    "# Evaluate XGBoost\n",
    "print(\"\\n2. XGBoost Test Set Metrics:\")\n",
    "test_results_xgb = evaluate_model(model_xgb, X_test, y_test)\n",
    "print(f\"  AUC: {test_results_xgb['auc']:.4f}\")\n",
    "print(f\"  Accuracy: {test_results_xgb['accuracy']:.4f}\")\n",
    "print(f\"\\n  Classification Report:\")\n",
    "print(classification_report(y_test, test_results_xgb['y_pred']))\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} {'Random Forest':<20} {'XGBoost':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'AUC':<25} {test_results_rf['auc']:<20.4f} {test_results_xgb['auc']:<20.4f}\")\n",
    "print(f\"{'Accuracy':<25} {test_results_rf['accuracy']:<20.4f} {test_results_xgb['accuracy']:<20.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. ROC Curve Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for both models\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure output_dir is defined (from Cell 1)\n",
    "if 'output_dir' not in globals():\n",
    "    from pathlib import Path\n",
    "    output_dir = Path('outputs')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Get ROC curves for both models\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, test_results_rf['y_pred_proba'])\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, test_results_xgb['y_pred_proba'])\n",
    "\n",
    "# Calculate AUC (already computed, but for display)\n",
    "auc_rf = test_results_rf['auc']\n",
    "auc_xgb = test_results_xgb['auc']\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.4f})', linewidth=2, color='#2E86AB')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.4f})', linewidth=2, color='#A23B72')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
    "plt.title('ROC Curve Comparison: Random Forest vs XGBoost', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "roc_curve_path = str(output_dir / 'roc_curve.png')\n",
    "plt.savefig(roc_curve_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ ROC curve saved to: {roc_curve_path}\")\n",
    "print(f\"\\nAUC Comparison:\")\n",
    "print(f\"  Random Forest: {auc_rf:.4f}\")\n",
    "print(f\"  XGBoost: {auc_xgb:.4f}\")\n",
    "print(f\"  Difference: {abs(auc_rf - auc_xgb):.4f} ({'RF' if auc_rf > auc_xgb else 'XGB'} is better)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Trading Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signals for both models on test set\n",
    "SIGNAL_THRESHOLD = 0.55  # Probability threshold for signal generation\n",
    "\n",
    "# Get actual returns for backtesting\n",
    "test_returns = data_features.loc[X_test.index, 'next_return'].dropna()\n",
    "\n",
    "# Generate signals for Random Forest\n",
    "signals_rf = pd.Series(\n",
    "    generate_signals(test_results_rf['y_pred_proba'], threshold=SIGNAL_THRESHOLD),\n",
    "    index=X_test.index\n",
    ")\n",
    "signals_rf = signals_rf.loc[test_returns.index]\n",
    "\n",
    "# Generate signals for XGBoost\n",
    "signals_xgb = pd.Series(\n",
    "    generate_signals(test_results_xgb['y_pred_proba'], threshold=SIGNAL_THRESHOLD),\n",
    "    index=X_test.index\n",
    ")\n",
    "signals_xgb = signals_xgb.loc[test_returns.index]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SIGNAL GENERATION - BOTH MODELS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRandom Forest Signal Statistics:\")\n",
    "print(f\"  Total signals: {signals_rf.sum()} out of {len(signals_rf)} days\")\n",
    "print(f\"  Signal rate: {signals_rf.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nXGBoost Signal Statistics:\")\n",
    "print(f\"  Total signals: {signals_xgb.sum()} out of {len(signals_xgb)} days\")\n",
    "print(f\"  Signal rate: {signals_xgb.mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTest returns range: {test_returns.index[0]} to {test_returns.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Backtest Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest both models\n",
    "print(\"=\"*70)\n",
    "print(\"BACKTEST PERFORMANCE - BOTH MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Backtest Random Forest\n",
    "print(\"\\n1. Random Forest Backtest:\")\n",
    "aligned_data_rf = pd.DataFrame({\n",
    "    'signals': signals_rf.values,\n",
    "    'returns': test_returns.values\n",
    "}, index=signals_rf.index)\n",
    "\n",
    "equity_rf, metrics_rf_bt = backtest_strategy(\n",
    "    aligned_data_rf['signals'].values,\n",
    "    aligned_data_rf['returns'].values,\n",
    "    initial_capital=10000.0\n",
    ")\n",
    "\n",
    "if isinstance(equity_rf, pd.Series):\n",
    "    equity_series_rf = equity_rf.reindex(aligned_data_rf.index)\n",
    "else:\n",
    "    equity_series_rf = pd.Series(equity_rf, index=aligned_data_rf.index)\n",
    "\n",
    "print(f\"  Total Return: {metrics_rf_bt['total_return_pct']:.2f}%\")\n",
    "print(f\"  Annualized Return: {metrics_rf_bt['annualized_return_pct']:.2f}%\")\n",
    "print(f\"  Volatility: {metrics_rf_bt['volatility_pct']:.2f}%\")\n",
    "print(f\"  Sharpe Ratio: {metrics_rf_bt['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {metrics_rf_bt['max_drawdown_pct']:.2f}%\")\n",
    "print(f\"  Win Rate: {metrics_rf_bt['win_rate_pct']:.2f}%\")\n",
    "print(f\"  Total Trades: {metrics_rf_bt['total_trades']}\")\n",
    "\n",
    "# Backtest XGBoost\n",
    "print(\"\\n2. XGBoost Backtest:\")\n",
    "aligned_data_xgb = pd.DataFrame({\n",
    "    'signals': signals_xgb.values,\n",
    "    'returns': test_returns.values\n",
    "}, index=signals_xgb.index)\n",
    "\n",
    "equity_xgb, metrics_xgb_bt = backtest_strategy(\n",
    "    aligned_data_xgb['signals'].values,\n",
    "    aligned_data_xgb['returns'].values,\n",
    "    initial_capital=10000.0\n",
    ")\n",
    "\n",
    "if isinstance(equity_xgb, pd.Series):\n",
    "    equity_series_xgb = equity_xgb.reindex(aligned_data_xgb.index)\n",
    "else:\n",
    "    equity_series_xgb = pd.Series(equity_xgb, index=aligned_data_xgb.index)\n",
    "\n",
    "print(f\"  Total Return: {metrics_xgb_bt['total_return_pct']:.2f}%\")\n",
    "print(f\"  Annualized Return: {metrics_xgb_bt['annualized_return_pct']:.2f}%\")\n",
    "print(f\"  Volatility: {metrics_xgb_bt['volatility_pct']:.2f}%\")\n",
    "print(f\"  Sharpe Ratio: {metrics_xgb_bt['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {metrics_xgb_bt['max_drawdown_pct']:.2f}%\")\n",
    "print(f\"  Win Rate: {metrics_xgb_bt['win_rate_pct']:.2f}%\")\n",
    "print(f\"  Total Trades: {metrics_xgb_bt['total_trades']}\")\n",
    "\n",
    "# Comprehensive Comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BACKTEST PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} {'Random Forest':<20} {'XGBoost':<20} {'Winner':<15}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Total Return (%)':<25} {metrics_rf_bt['total_return_pct']:<20.2f} {metrics_xgb_bt['total_return_pct']:<20.2f} {('RF' if metrics_rf_bt['total_return_pct'] > metrics_xgb_bt['total_return_pct'] else 'XGB'):<15}\")\n",
    "print(f\"{'Annualized Return (%)':<25} {metrics_rf_bt['annualized_return_pct']:<20.2f} {metrics_xgb_bt['annualized_return_pct']:<20.2f} {('RF' if metrics_rf_bt['annualized_return_pct'] > metrics_xgb_bt['annualized_return_pct'] else 'XGB'):<15}\")\n",
    "print(f\"{'Sharpe Ratio':<25} {metrics_rf_bt['sharpe_ratio']:<20.2f} {metrics_xgb_bt['sharpe_ratio']:<20.2f} {('RF' if metrics_rf_bt['sharpe_ratio'] > metrics_xgb_bt['sharpe_ratio'] else 'XGB'):<15}\")\n",
    "print(f\"{'Max Drawdown (%)':<25} {metrics_rf_bt['max_drawdown_pct']:<20.2f} {metrics_xgb_bt['max_drawdown_pct']:<20.2f} {('RF' if metrics_rf_bt['max_drawdown_pct'] > metrics_xgb_bt['max_drawdown_pct'] else 'XGB'):<15}\")\n",
    "print(f\"{'Win Rate (%)':<25} {metrics_rf_bt['win_rate_pct']:<20.2f} {metrics_xgb_bt['win_rate_pct']:<20.2f} {('RF' if metrics_rf_bt['win_rate_pct'] > metrics_xgb_bt['win_rate_pct'] else 'XGB'):<15}\")\n",
    "print(f\"{'Total Trades':<25} {metrics_rf_bt['total_trades']:<20} {metrics_xgb_bt['total_trades']:<20} {'-':<15}\")\n",
    "\n",
    "# Determine overall best model based on Sharpe ratio (risk-adjusted return)\n",
    "if metrics_rf_bt['sharpe_ratio'] > metrics_xgb_bt['sharpe_ratio']:\n",
    "    best_backtest_model = 'Random Forest'\n",
    "    best_equity_series = equity_series_rf\n",
    "    best_metrics_bt = metrics_rf_bt\n",
    "elif metrics_xgb_bt['sharpe_ratio'] > metrics_rf_bt['sharpe_ratio']:\n",
    "    best_backtest_model = 'XGBoost'\n",
    "    best_equity_series = equity_series_xgb\n",
    "    best_metrics_bt = metrics_xgb_bt\n",
    "else:\n",
    "    # If Sharpe is equal, use total return\n",
    "    if metrics_rf_bt['total_return_pct'] > metrics_xgb_bt['total_return_pct']:\n",
    "        best_backtest_model = 'Random Forest'\n",
    "        best_equity_series = equity_series_rf\n",
    "        best_metrics_bt = metrics_rf_bt\n",
    "    else:\n",
    "        best_backtest_model = 'XGBoost'\n",
    "        best_equity_series = equity_series_xgb\n",
    "        best_metrics_bt = metrics_xgb_bt\n",
    "\n",
    "print(f\"\\n✓ Best model for backtesting (by Sharpe Ratio): {best_backtest_model}\")\n",
    "print(f\"  Sharpe Ratio: {best_metrics_bt['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Total Return: {best_metrics_bt['total_return_pct']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot equity curves for both models\n",
    "# Ensure output_dir is defined (from Cell 1)\n",
    "if 'output_dir' not in globals():\n",
    "    from pathlib import Path\n",
    "    output_dir = Path('outputs')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Plot Random Forest equity curve\n",
    "equity_curve_path_rf = str(output_dir / 'equity_curve_rf.png')\n",
    "plot_equity_curve(equity_series_rf, equity_curve_path_rf, 'Random Forest Strategy Equity Curve')\n",
    "\n",
    "# Plot XGBoost equity curve\n",
    "equity_curve_path_xgb = str(output_dir / 'equity_curve_xgb.png')\n",
    "plot_equity_curve(equity_series_xgb, equity_curve_path_xgb, 'XGBoost Strategy Equity Curve')\n",
    "\n",
    "# Plot best model (for compatibility with existing code)\n",
    "equity_curve_path = str(output_dir / 'equity_curve.png')\n",
    "plot_equity_curve(best_equity_series, equity_curve_path, f'{best_backtest_model} Strategy Equity Curve (Best)')\n",
    "\n",
    "print(f\"\\n✓ Equity curves saved:\")\n",
    "print(f\"  - Random Forest: {equity_curve_path_rf}\")\n",
    "print(f\"  - XGBoost: {equity_curve_path_xgb}\")\n",
    "print(f\"  - Best Model ({best_backtest_model}): {equity_curve_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
